{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "## Independent Expenditures\n",
    "### What is an Independent Expenditure?\n",
    "According to the <a href='https://www.fec.gov/help-candidates-and-committees/making-disbursements-pac/independent-expenditures-nonconnected-pac/'> Federal Election Commission documentation</a>, an Independent Expenditure is an expenditure for a communication that 'expressly advocates the election or defeat of a clearly identified federal candidate; and is not coordinated with a candidate, candidateâ€™s committee, party committee or their agents.'\n",
    "\n",
    "Political action committees make independent expenditures to support or opposed candidates.  Independent expenditures are not contributions to a candidate and are therefore not subject to contribution limits.\n",
    "\n",
    "In this analysis we are finding the similarity between candidates, based on those committees which spend money to support or oppose them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "## Calculating the Similarity Coefficient between candidates based on their shared contributors\n",
    "### Overview\n",
    "In this notebook we calculate the <a href=https://en.wikipedia.org/wiki/Jaccard_index> 'Jaccard Index', or Similarity Coefficient </a>, of candidates based on the identities of the committees who have spent money (independent expenditures) to support or oppose them.\n",
    "\n",
    "Scores are between 0 and 100, with 100 being identical set of committee spenders and 0 being no shared spenders.  For example: two candidates have no similarity in terms of expenditures on them, they will have a Jaccard Index of 0.  Two candidates who have exactly the same committees spending money on them will have a Jaccard Index of 100.\n",
    "\n",
    "This is useful in understanding patterns of giving and opposition, and how political action committees might be exercising influence across multiple candidates and races.\n",
    "\n",
    "### Method\n",
    "1. Obtain an RDD of the form (CandidateID, [list of contributor IDs])\n",
    "2. Cross join of the RDD generated in step 1 with itself\n",
    "3. Calculate the Jaccard Index of each set produced by the cross join\n",
    "    - get the sum of the length of the sets of contributors for each candidate being compared\n",
    "    - get the length of the union of the sets of contributors to each candidate\n",
    "    - divide the union of the sets by the total length of the sets\n",
    "    - multiple by 100\n",
    "4. Return an RDD of the form (Jaccard Index, (Candidate1ID, Candidate2ID))\n",
    "5. Print the return to the console\n",
    "\n",
    "#### Author\n",
    "Dan Budris <d.c.budris@gmail.com>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "# Basic modules\n",
    "import re\n",
    "import datetime\n",
    "from operator import add\n",
    "\n",
    "# Data analysis modules\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate variables for processing data moving forward\n",
    "spark = SparkSession.builder.appName('ElectionAnalyzer').getOrCreate()\n",
    "datapath = '/Users/Dan/data/'\n",
    "\n",
    "# Set the election year; modify this value to look at a different year\n",
    "election_year = \"2016\"\n",
    "\n",
    "# Set the file paths for the data file, based on the data path and election year\n",
    "independent_expenditure_file = '{0}independent_expenditure_{1}.csv'.format(datapath, election_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV as a spark dataframe\n",
    "ind_exp = spark.read.csv(independent_expenditure_file, header=True)\n",
    "\n",
    "# Convert the dataframe to an RDD, and \n",
    "# filter out the empty lines\n",
    "total_expenditures_rdd = (\n",
    "    ind_exp\n",
    "    .rdd\n",
    "    .filter(lambda x: len(x) != 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for preping and analyzing the data \n",
    "\n",
    "def prep_data(input_rdd, val_key, val_value):\n",
    "    ''' A function to map and reduce an RDD based on a given key and value.\n",
    "        Provided an object attribute to use as a key, and one to use as a value,\n",
    "        will map the rdd to a tuple of (key, value) and then reduce by key by \n",
    "        addition\n",
    "    '''\n",
    "    _prepped_data = (\n",
    "        input_rdd\n",
    "        .filter(lambda x: (getattr(x, val_key) != 'None'))\n",
    "        .map(lambda x: (getattr(x, val_key), [getattr(x, val_value)]))\n",
    "        .reduceByKey(lambda x, y: x + y)\n",
    "        .map(lambda x: (x[0], set(x[1])))\n",
    "    )\n",
    "    return _prepped_data\n",
    "    \n",
    "def calculate_similarity_coefficient(set_one, set_two, min_similar=None):\n",
    "    ''' Function to calculate the Jaccard Index, given two sets\n",
    "        Returns the Jaccard Index as a numeric percentage between 0 and 100\n",
    "        Given a 'min_similar' value, will only return positive values for those \n",
    "        sets whose intersection is greater than or equal to the minimum similarity.\n",
    "    '''\n",
    "    _set_one = set(set_one)\n",
    "    _set_two = set(set_two)\n",
    "    _set_union_len = len(_set_one.union(_set_two))\n",
    "    _set_intersect_len = len(_set_one.intersection(_set_two))\n",
    "    if min_similar:\n",
    "        if _set_intersect_len < min_similar:\n",
    "            return 0\n",
    "    _sim_coeff = _set_intersect_len/_set_union_len * 100\n",
    "    return _sim_coeff\n",
    "    \n",
    "def rdd_similarity_coefficient_map(input_rdd):\n",
    "    ''' Given an RDD of the form [(Identifier, [List of Characteristics])]\n",
    "        This function will produce an RDD of the form [(Jaccard Index), Identifier1, Identifier2]\n",
    "        based on the cartesian product of the RDD\n",
    "    '''\n",
    "    _coeff_sim = (\n",
    "        input_rdd\n",
    "        # get the cartesian product of the input rdd with itself\n",
    "        .cartesian(input_rdd)\n",
    "        # filter out the self * self products\n",
    "        .filter(lambda x: x[0] != x[1])\n",
    "        # calculate the Jaccard Index for each result of the cross-product\n",
    "        # returning a nested tuple of (Jaccard Index, (cand1 id, cand2 id))\n",
    "        .map(lambda x: (calculate_similarity_coefficient(x[0][1], x[1][1], min_similar=5), (x[0][0], x[1][0])))\n",
    "    )\n",
    "    return _coeff_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Jaccard similarity for each candidate the committee who has spent on that candidate\n",
    "candidate_pairs_with_jaccard_index = (\n",
    "    rdd_similarity_coefficient_map(\n",
    "        prep_data(\n",
    "            total_expenditures_rdd, 'cand_id', 'spe_id'\n",
    "        )\n",
    "    )\n",
    "    #.map(lambda x: (x[0], frozenset([x[1][0], x[1][1]])))\n",
    "    #.distinct()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 270, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1556, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'cand_id' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 370, in func\n    return f(iterator)\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 1876, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 237, in mergeValues\n    for k, v in iterator:\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-25-ad98b0cd20ac>\", line 11, in <lambda>\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1561, in __getattr__\n    raise AttributeError(item)\nAttributeError: cand_id\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1556, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'cand_id' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 370, in func\n    return f(iterator)\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 1876, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 237, in mergeValues\n    for k, v in iterator:\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-25-ad98b0cd20ac>\", line 11, in <lambda>\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1561, in __getattr__\n    raise AttributeError(item)\nAttributeError: cand_id\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f29d966acea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Collect the (similarity, candidate pair) RDD, printing to the console\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcandidate_pairs_with_jaccard_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \"\"\"\n\u001b[1;32m    833\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 270, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1556, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'cand_id' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 370, in func\n    return f(iterator)\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 1876, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 237, in mergeValues\n    for k, v in iterator:\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-25-ad98b0cd20ac>\", line 11, in <lambda>\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1561, in __getattr__\n    raise AttributeError(item)\nAttributeError: cand_id\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1556, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'cand_id' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 370, in func\n    return f(iterator)\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/rdd.py\", line 1876, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 237, in mergeValues\n    for k, v in iterator:\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-25-ad98b0cd20ac>\", line 11, in <lambda>\n  File \"/Users/Dan/Desktop/stagingArea/FEC_data_notebooks/FEC_data_notebook/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1561, in __getattr__\n    raise AttributeError(item)\nAttributeError: cand_id\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Collect the (similarity, candidate pair) RDD, printing to the console\n",
    "candidate_pairs_with_jaccard_index.sortByKey(False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
